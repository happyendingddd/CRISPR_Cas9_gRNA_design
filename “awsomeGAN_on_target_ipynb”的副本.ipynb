{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“awsomeGAN_on_target.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPfkKNllSA1KgTbqRt3zRLt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/happyendingddd/CRISPR_Cas9_gRNA_design/blob/main/%E2%80%9CawsomeGAN_on_target_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC1wMaVNv5DV"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83x53-eHv-Xw"
      },
      "source": [
        "file_path='/content/hek293t_negetive.csv'\n",
        "data_read=pd.read_csv(file_path,sep=',')\n",
        "guideSeq=np.array(data_read['sgRNA'])\n",
        "#labels=np.array(data_read['Normalized_efficacy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfopnsTCv-aV"
      },
      "source": [
        "ntmap = {'A': (1, 0, 0, 0),\n",
        "         'C': (0, 1, 0, 0),\n",
        "         'G': (0, 0, 1, 0),\n",
        "         'T': (0, 0, 0, 1)\n",
        "         }\n",
        "ntmap_new={}\n",
        "for key,val in ntmap.items():\n",
        "    ntmap_new[val]=key\n",
        "\n",
        "def get_seqcode(seq):\n",
        "    return list(map(lambda c: ntmap[c], seq))\n",
        "\n",
        "def oneHotcoding(Seq):\n",
        "    n=0\n",
        "    for seq in Seq:\n",
        "        if n==0:\n",
        "            SeqcodeL=[]\n",
        "        seqcode=get_seqcode(seq)\n",
        "        n+=1\n",
        "        SeqcodeL.append(seqcode)\n",
        "        seqcode=[]\n",
        "        SeqcodeA=np.array(SeqcodeL)\n",
        "    return SeqcodeA\n",
        "\n",
        "guidecode=oneHotcoding(guideSeq)\n",
        "train_data=guidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrs30kYDv-d4"
      },
      "source": [
        "a=np.reshape(train_data,(train_data.shape[0],1,23,4))\n",
        "train_data=tf.convert_to_tensor(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXXCLyepwICr"
      },
      "source": [
        "BUFFER_SIZE = train_data.shape[0]\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# 批量化和打乱数据\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW7pegpLwLsL"
      },
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(5*1, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((5,1)))\n",
        "    assert model.output_shape == (None, 5,1) # 注意：batch size 没有限制\n",
        "\n",
        "    model.add(layers.Conv1DTranspose(128,5, strides=1, padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 5, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv1DTranspose(64,5, strides=2, padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 10, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv1DTranspose(4, 5, strides=3, padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 30, 4)\n",
        "\n",
        "    model.add(layers.Conv1D(4,8,strides=1, padding='valid', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 23, 4)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUk2fK6IwQBw"
      },
      "source": [
        "generator = make_generator_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK0N3aiwwQHW"
      },
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    \n",
        "    model.add(layers.Conv1D(16, 3, strides=1, padding='same',input_shape=(23, 4)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv1D(32,3, strides=1, padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgEQyz5gwXNt"
      },
      "source": [
        "discriminator = make_discriminator_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38d5pbhMwXUL"
      },
      "source": [
        "# 该方法返回计算交叉熵损失的辅助函数\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYFr9TvLwbKi"
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvwYzeiawbQz"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_Njx6hKwf6c"
      },
      "source": [
        "EPOCHS = 50\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 5000\n",
        "\n",
        "# 将重复使用该种子\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maK9QNEcyjzG"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\") # 连接两个或更多的路径名组件\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                  discriminator_optimizer=discriminator_optimizer,\n",
        "                  generator=generator,\n",
        "                  discriminator=discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yTbYXSMwkYM"
      },
      "source": [
        "def train_step(sequences):\n",
        "  noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    generated_sequences = generator(noise, training=True)\n",
        "      \n",
        "    real_output = discriminator(sequences, training=True)\n",
        "    fake_output = discriminator(generated_sequences, training=True)\n",
        "\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "    disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puVHtYM2BEiZ"
      },
      "source": [
        "# 对应到序列\n",
        "ntmap_new={}\n",
        "for key,val in ntmap.items():\n",
        "    ntmap_new[val]=key\n",
        "\n",
        "def get_seq(seqcode): # 只能处理个碱基\n",
        "    for code in ntmap_new:\n",
        "        if (seqcode==code).all():\n",
        "            seq=ntmap_new[code]\n",
        "    return seq\n",
        "\n",
        "def oneHotdecoding(codes): # 只能处理一条序列\n",
        "    n=0\n",
        "    for code in codes:\n",
        "        if n==0:\n",
        "            SeqL=[]\n",
        "        seq=get_seq(code)\n",
        "        n+=1\n",
        "        SeqL.append(seq)\n",
        "        seq=[]\n",
        "        SeqA=np.array(SeqL)\n",
        "    return SeqA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsoKP1YXBBtX"
      },
      "source": [
        "def savedata1D(list1,filepath):\n",
        "    output = open(filepath,'a+',encoding='utf-8')\n",
        "    #output.write(string)\n",
        "    #output.write('\\n')\n",
        "    for i in range(len(list1)):\n",
        "        output.write(str(list1[i]))    \n",
        "    output.write('\\n')           \n",
        "    output.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eji-nY6wo5Q"
      },
      "source": [
        "def generate_and_save_sequence(model, epoch, test_input,save=False):\n",
        "  # 注意 training` 设定为 False\n",
        "  # 因此，所有层都在推理模式下运行（batchnorm）\n",
        "  predictions = model(test_input, training=False)\n",
        "  for i in range(predictions.shape[0]):\n",
        "    #print(predictions[i,:,:])\n",
        "    list1=[]\n",
        "    temp=predictions[i,:,:].numpy() # 急切执行默认情况下.numpy()处于启用状态，因此只需调用Tensor对象即可\n",
        "    temp2=np.zeros(temp.shape)\n",
        "    for i in range(temp.shape[0]):\n",
        "      for j in range(temp.shape[1]):\n",
        "        if temp[i][j]==np.amax(temp[i]):\n",
        "          temp2[i][j]=1\n",
        "        else:\n",
        "          temp2[i][j]=0\n",
        "    print(temp2)\n",
        "    list1.append(temp2)\n",
        "    if save:\n",
        "      new_codes=np.stack(list1, axis=0)\n",
        "      guideseqs=[]\n",
        "      for codes in new_codes:\n",
        "        guideseqs.append(oneHotdecoding(codes))\n",
        "      for seq in guideseqs:\n",
        "        fp='/content/generated_seq_hek293t_negetive.txt'\n",
        "        savedata1D(seq,fp)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkQa_eY4AroG"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for sequence_batch in dataset:\n",
        "      train_step(sequence_batch) # 计算训练集中每一个batch对应的step\n",
        "      \n",
        "    # 每 15 个 epoch 保存一次模型（模型参数）\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start)) # 每训练一次打印一次训练时长\n",
        "\n",
        "  # 最后一个 epoch 结束后清除输出并生成保存序列\n",
        "  display.clear_output(wait=True) # 清除输出\n",
        "  generate_and_save_sequence(generator, # 生成保存序列\n",
        "              epochs,\n",
        "              seed,save=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYZNoSujwqUo"
      },
      "source": [
        "%%time\n",
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tnq_rOZ-y2GZ",
        "outputId": "b9d7d90a-3c8e-4a03-ad74-6fc4258694da"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f5b174e5050>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    }
  ]
}